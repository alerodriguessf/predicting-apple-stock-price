{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPilloRUdanm41pQvbMXY2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alerodriguessf/predicting-apple-stock-price/blob/main/Portfolio_Predicting_Apple_Stock_Price_SARIMAX_20250114.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Apple Stock Price using SARIMAX"
      ],
      "metadata": {
        "id": "fNoMHAvUfuLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Importing Necessary Libraries\n",
        "### The following libraries are required for data manipulation, visualization, and modeling."
      ],
      "metadata": {
        "id": "unBZF4-RT7ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "RB4DSS-nWlxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XcbQN7551JW"
      },
      "outputs": [],
      "source": [
        "from pmdarima.arima import auto_arima\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tools.eval_measures import mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Data Acquisition"
      ],
      "metadata": {
        "id": "ow6_Wf63Ul4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the dataset through the Google Colab file upload functionality.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "Y4A0VXJNfzZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Loading the Dataset\n"
      ],
      "metadata": {
        "id": "a60DPSGsUrXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We load the dataset into a Pandas DataFrame for analysis. This dataset contains stock prices for Apple.\n",
        "\n",
        "df = pd.read_excel('price_apple.xlsx')"
      ],
      "metadata": {
        "id": "eae0CRvgMgd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first 10 rows of the dataset to understand its structure.\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "CGBHhdEuMsQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2sw_Dt-lUdnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Feature Engineering\n"
      ],
      "metadata": {
        "id": "o9fbOrA5UvTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a new feature 'mean' that calculates the average of the 'Low' and 'High' prices for each day.\n",
        "\n",
        "df['mean'] = (df['Low'] + df['High'])/2"
      ],
      "metadata": {
        "id": "sgfbcC6CM0Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the updated DataFrame with the new 'mean' column.\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "TRHTBBG4QsXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Shifting the Target Variable for Prediction\n"
      ],
      "metadata": {
        "id": "id3Mihb0U9OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We shift the 'mean' column by -1 to create a column 'Actual' representing the target variable for prediction.\n",
        "# We do this to model the future price (next day's average price).\n",
        "\n",
        "steps = -1\n",
        "df_pred = df.copy()\n",
        "df_pred['Actual'] = df_pred['mean'].shift(steps)\n",
        "df_pred.head()"
      ],
      "metadata": {
        "id": "NpMk-sfOc-i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Cleaning the Data\n"
      ],
      "metadata": {
        "id": "H3l9lzjyVOD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping any rows with missing values due to the shift operation, ensuring data consistency for model training.\n",
        "\n",
        "df_pred = df_pred.dropna()"
      ],
      "metadata": {
        "id": "E_WE90IsdatY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Converting Date Column to Datetime and Setting as Index\n"
      ],
      "metadata": {
        "id": "-j0waHQ-VW_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'Date' to a datetime object and setting it as the index of the DataFrame for easier time-series manipulation.\n",
        "\n",
        "df_pred['Date'] = pd.to_datetime(df_pred['Date'])\n",
        "df_pred.index = df_pred['Date']"
      ],
      "metadata": {
        "id": "DBZ2n4Prdd1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Visualizing the 'mean' Column\n"
      ],
      "metadata": {
        "id": "WWscLRhiVd6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the 'mean' column to get an initial understanding of the data's trend and seasonality.\n",
        "\n",
        "df_pred[\n",
        "    'mean'\n",
        "    ].plot(figsize = (15, 2))"
      ],
      "metadata": {
        "id": "oMX5POyxkuFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Seasonal Decomposition of the Time Series\n"
      ],
      "metadata": {
        "id": "7Lwk5riwViU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decomposing the 'mean' series using an additive model to understand its seasonal, trend, and residual components.\n",
        "\n",
        "sd = sm.tsa.seasonal_decompose(df_pred['mean'], model = 'additive', period = 365)\n",
        "sd.plot()"
      ],
      "metadata": {
        "id": "QyYTucnEk5GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Feature Scaling\n"
      ],
      "metadata": {
        "id": "xo6EpktYVzHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use MinMaxScaler to normalize the features to a range between 0 and 1 for both input features (X) and target variable (Y).\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_input = scaler.fit_transform(df_pred[['Low', 'High', 'Close', 'Adj Close', 'Volume','mean']])\n",
        "scaler_input = pd.DataFrame(scaler_input)\n",
        "x = scaler_input # Assigning the scaled values to the input features (X)\n"
      ],
      "metadata": {
        "id": "QnJsQOAIlQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_output = scaler.fit_transform(df_pred[['Actual']]) # Scaling the target variable (Actual)\n",
        "scaler_ouput = pd.DataFrame(scaler_output)\n",
        "y = scaler_output # Assigning the scaled values to the target variable (Y)"
      ],
      "metadata": {
        "id": "vYaT55ELmLVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming columns for clarity\n",
        "x.rename(columns = {0: 'Low', 1: 'High', 2: 'Close', 3: 'Adj Close', 4: 'Volume', 5: 'mean'}, inplace = True)\n",
        "x.index = df_pred.index\n",
        "x.head()"
      ],
      "metadata": {
        "id": "W7PSsASdmaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.DataFrame(scaler_output)\n",
        "y.rename(columns = {0:'stock_price'}, inplace = True) # Renaming the target variable 'Preço_açao' (Stock Price)\n",
        "y.index = df_pred.index\n",
        "y.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tIMfQcGFmklj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Splitting the Data into Training and Test Sets\n"
      ],
      "metadata": {
        "id": "AJIfVYjZWasS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the data into training and test sets (70% for training, 30% for testing) to evaluate the model's performance.\n",
        "\n",
        "train_size = int(len(x) * 0.70)\n",
        "test_size = int(len(df_pred)) - train_size\n",
        "train_size, test_size"
      ],
      "metadata": {
        "id": "KWDtbbiHnKpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the training and testing sets for both input features (X) and target variable (Y)\n",
        "\n",
        "train_x, train_y = x[:train_size], y[:train_size]\n",
        "test_x, test_y = x[train_size:].dropna(), y[train_size:]"
      ],
      "metadata": {
        "id": "5RO_NW-2ngL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 12: Automatic ARIMA Model Selection\n"
      ],
      "metadata": {
        "id": "8Wcyn1kiWjVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the auto_arima function from pmdarima to find the best parameters for the ARIMA model.\n",
        "\n",
        "step_wise = auto_arima(train_y, exogenous = train_x,\n",
        "                       trace = True,\n",
        "                       start_p=1,\n",
        "                       start_q=1,\n",
        "                       max_p=7,\n",
        "                       max_q=7,\n",
        "                       d=1,\n",
        "                       max_d= 7,\n",
        "                       stepwise= True\n",
        "                                              )"
      ],
      "metadata": {
        "id": "OvM9g7vKn_Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the summary of the best model found by auto_arima\n",
        "\n",
        "step_wise.summary()"
      ],
      "metadata": {
        "id": "Z4ezHt_so9Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Log-Likelihood**: 7569.047 indicates a good model fit.\n",
        "AIC: -15132.095, BIC: -15115.391, and HQIC: -15125.952 all suggest the model is appropriate, with low values favoring good fit and penalizing overfitting.\n",
        "\n",
        "**Coefficient Estimates:**\n",
        "\n",
        "Intercept: 0.0002, though not statistically significant (p-value 0.157).\n",
        "MA Term (ma.L1): 0.1723, highly significant (p-value < 0.0001), indicating that past errors significantly influence future stock prices.\n",
        "Variance of Residuals (sigma2): 2.339e-05, with a z-score of 75.974, suggesting a good model fit with small residual variance.\n",
        "\n",
        "**Model Diagnostics:**\n",
        "\n",
        "Ljung-Box Test: p-value 0.97 shows no significant autocorrelation, indicating white noise residuals.\n",
        "Jarque-Bera Test: p-value 0.00, rejecting normality and suggesting non-normal residuals.\n",
        "Heteroskedasticity: p-value 0.00 indicates variance instability, implying heteroskedasticity in the model.\n",
        "Skew: -0.37 suggests slight left-skewness in residuals.\n",
        "Kurtosis: 13.66 indicates heavy-tailed residuals.\n",
        "\n",
        "**Conclusion:** The SARIMAX model shows strong performance, with significant model parameters and good fit. However, residuals display non-normality and heteroskedasticity, which may require further adjustments to improve predictive accuracy and model robustness."
      ],
      "metadata": {
        "id": "bDWFd9VTblY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting input features (X) and target variable (Y) to numpy arrays for SARIMAX compatibility\n",
        "\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)"
      ],
      "metadata": {
        "id": "ONFULUs_pE1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the SARIMAX model with the best found parameters\n",
        "\n",
        "model = SARIMAX(\n",
        "    train_y,\n",
        "    exog = train_x,\n",
        "    order = (0, 1, 1),\n",
        "    enforce_invertibility= False,\n",
        "    enforce_stationarity= False\n",
        ")"
      ],
      "metadata": {
        "id": "YbKSpKOzpYTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the SARIMAX model to the training data\n",
        "results = model.fit()"
      ],
      "metadata": {
        "id": "t5XDbM18pp5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 13: Making Predictions\n"
      ],
      "metadata": {
        "id": "7faWoJaNqBtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We predict future values using the fitted model on the test set.\n",
        "\n",
        "pred = results.predict(start=train_size, end=train_size + test_size + (steps), exog=test_x)"
      ],
      "metadata": {
        "id": "mpnjPZfmprtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 14: Comparing Predictions with Actual Values\n"
      ],
      "metadata": {
        "id": "R9UZtt_1XvCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a DataFrame for the actual stock prices and predictions to compare and evaluate the model.\n",
        "\n",
        "act = pd.DataFrame(scaler_output[train_size:, 0])\n",
        "act.index = test_x.index\n",
        "act.rename(columns={0: 'Preço_açao'}, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6-Zr-x_kqq0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organizing predictions for easy comparison\n",
        "\n",
        "pred =pd.DataFrame(pred)\n",
        "pred.reset_index(drop=True, inplace=True)\n",
        "pred.index=test_x.index\n",
        "pred['Actual'] = act['Preço_açao']\n",
        "pred.rename(columns={0:'Predicted'}, inplace=True)\n",
        "pred.head()"
      ],
      "metadata": {
        "id": "6WbfuUOHshP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 17: Visualizing Actual vs. Predicted Values\n"
      ],
      "metadata": {
        "id": "NNU90SvHX6-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We plot both the actual and predicted stock prices for visual comparison.\n",
        "\n",
        "pred['Actual'].plot(figsize=(20,8), legend = True, color = 'blue')\n",
        "pred['Predicted'].plot(legend= True, color='red', figsize=(20,8))"
      ],
      "metadata": {
        "id": "U79edro0tZef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 18: Model Evaluation\n"
      ],
      "metadata": {
        "id": "l6wfxvzgYEjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the MSE to assess the model's performance\n",
        "\n",
        "error = mse(pred['Actual'], pred['Predicted'])\n",
        "error"
      ],
      "metadata": {
        "id": "KcNMWROIu1qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE value of 0.000150831116141754 reflects that the SARIMAX model has a relatively small prediction error."
      ],
      "metadata": {
        "id": "viVEsftiapeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the MRSE to assess the model's performance\n",
        "\n",
        "mrse = np.sqrt(error)\n",
        "mrse"
      ],
      "metadata": {
        "id": "x72wc2HnaFpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MRSE of 0.01228 confirms that the SARIMAX model has relatively small prediction errors."
      ],
      "metadata": {
        "id": "GZ-1aQn3a7jC"
      }
    }
  ]
}